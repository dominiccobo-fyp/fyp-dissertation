\chapter{Iterations}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{iterations-diagram.png}
	\caption{Brief overview of the contents covered in each iteration}
\end{figure}

\newpage
\section{Establishing the Architectural Plumbing}

There are two choices when it comes to creating modular applications, either a plugin based system, such as Javaâ€™s OSGi standard, or independent applications that communicate over some form of local or remote connection.

Naturally, this poses questions as to what the nature of scale of the problem would be; in our case, tying multiple information sources together, typical of a medium size corporation do we lean to the solution that better applies to a larger scale.

Extensibility in our case is important; there is an inexorable need for addition of new sources. Plugins are the inherent approach to providing modular extension to a monolithic architecture (Figure \ref{fig:monolithic-approach}). Whereas, as a superset of Service Oriented Architecture, Micro-services are more strongly decoupled (Figure \ref{fig:distributed-approach}), relying on middleware to communicate based on some form of predefined contract.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{monolithic-approach.png}
	\caption{Component Diagram Illustrating Monolithic Approach}
	\label{fig:monolithic-approach}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{distributed-application.png}
	\caption{Component Diagram Illustrating Distributed Approach}
	\label{fig:distributed-approach}
\end{figure}

With large amounts of data exchanged - as is the potential in the envisioned system - a single monolithic application acting as the single processor of the data can rapidly become the chokepoint, subject to temporary mediation via vertical scaling. In contrast, these problems are easily mitigated in distributed architecture through horizontal scaling.

Microservices are no silver bullet approach to proverbial modularity and, or business scalability. In fact, they are often associated with greater development overhead, especially, in the aspect of integration and testing due to lack of immediate feedback on changes; the appropriation of the correct culture, practises and tooling can mitigate many of these overheads. Failing these appropriations, other common anti patterns may emerge, such as distributed monoliths. We will discuss these later.

There are many integration patterns for distributed systems. EIP - a referential principle-driven manual - notes their shortcomings and benefits, but strongly recommends messaging integration patterns as the best approach for most Enterprise problems favouring them based on their high frequency, asynchronous capability.

Implementing a fully functional, distributed messaging solution is complex endeavour; reinventing the wheel is futile when there are ample choices to choose from; RabbitMQ, Apache Kafka, to name a few. Not withstanding the reduction of unnecessary development effort, from the elimination of implementing middleware, duct tape code is still necessary to handle the endpoints connecting to these.

The JVM and its associated languages, are ubiquitous in back office systems in many businesses across the globe; this maturity yields stage to many out-of-the-box solutions to common enterprise problems such as that which we have discussed. Spring is an example of one of the most widely adopted enterprise Java frameworks facilitating these aids.

Spring itself does not however dictate the style of distributed programming but offers a strongly opinionated view on the approaches available. We chose the Command Query Responsibility Segregation (CQRS) approach based on its focus on separation of updating a set of projections - query models - and updating of a central decoupled model - command models. Implementing this pattern is infamously difficult, but with help of an extension based on Spring - Axon Framework - can we reduce much of this effort.  

With the choice of Axon come some beneficial consequences. The framework provides a strongly opinionated view of what is best for the developer; its strong point lays in its approach to Messaging. It defines its own Message Bus implementation - which can be substituted from a set of other choices if needed - as well as handling all of the boiler-plating that would otherwise be needed to glue message endpoint consumption together.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{distributed-approach-with-axon.png}
	\caption{Component Diagram Illustrating Distributed Approach with Axon Framework}
	\label{fig:distributed-approach}
\end{figure}

\section{Establishing a Retrieval Mechanism}

Having established, through thorough evaluation, a sound architecture, we proceed to establishing our approach to how data is expected to flow between the defined components. Having defined, and implemented a rough architecture above, we resort to implementing a minimum viable product for retrieving from a client for which we choose a HTTP Web Approach with a single test parameter in the pursuit of a creation of query handler that will return a basic hello world message returning the test parameter appended. Following this approach, we are able to create a very simple test case to verify the functionality (Figure \ref{fig:e2eMockTest}).

\begin{figure}[h!]
		\centering
		\lstinputlisting[language=Python, showspaces=false,                
		showstringspaces=false, tabsize=2]{e2e-acceptance-test-1.py}
		\caption{End To End Test for Mock Example}
		\label{fig:e2eMockTest}
\end{figure}

To achieve the functionality, we define a query object capable of transmitting the test payload. The query object also serves as the contract for those wishing to act on it based on its object signature. We model this process in Figure. Clients receive a query through an interface which they dispatch with context to the message bus and which is then returned from the appropriate source. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{first_e2e_test_class_uml.png}
	\caption{UML Class Diagram of the models involved in building and responding to a query}
	\label{fig:firste2etestclassuml}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{process-diagram.png}
	\caption{Process diagram illustrating flow of query from its handling to response}
	\label{fig:processSequenceQueryFlow}
\end{figure}

By implementing this approach, we end up with a core code example of what will form the implementation pattern throughout this project (Figure \ref{fig:e2eMockClass}). The simplicity of this approach is evident and strongly contributes to our goal of ease of extensibility. The complexity of deploying this distributed application is handled through the use of containerisation - Docker - and container orchestration - Docker Compose. 

Despite not performing much processing, the response times indicate a positive approach to reducing the time taken to retrieve information.

% TODO: insert graph here.

% TODO: what are the response times like here?

\begin{figure}[h!]
	\centering
	\lstinputlisting[language=Java, showspaces=false,                
	showstringspaces=false, tabsize=2, breaklines=true]{mock-e2e.java}
	\caption{End To End Implementation for Mock Example}
	\label{fig:e2eMockClass}
\end{figure}

\section{Connecting an IDE - Evaluating the Push Model}

In the previous section, we successfully managed to establish a basic decoupled communication model that enabled the easy integration of information from a given source. In line with the background research, we have a key interest in reducing the amount of context switching between multiple sources performed by a developer and as such found the IDE to be a suitable approach. We also explored the relevance of simple problems such as the Language Server Protocol. 

We now explore the extension of our current solution in pursuit of the ability to connect it to an IDE. In this case, Visual Studio Code. 

When thinking of creating reusable IDE Intelligence to enrich context based on a given syntax and or grammar, we consider the Language Server protocol. 

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=1\linewidth]{push_trial_story.png}
		\caption{User Story}
	\end{subfigure}
	\begin{subfigure}[t]{0.6\textwidth}
		\includegraphics[width=1\linewidth]{push-wireframe}
		\caption{Wireframe}
	\end{subfigure}
	\label{fig:mvpPush}
	\caption{Minimum Viable Product Illustration for Push Approach}
\end{figure}

Examining examples of its usage, such as Eclipse's Java LSP implementation, which uses the Eclipse Java Development Tools to provide Java support to all LSP supporting IDEs.

\begin{figure}[h!]
	
	\digraph[width=\textwidth]{eclipseLanguageServer}{rankdir=LR; 
		IDE->languageServer [label="Push Ctx"];
		languageServer->IDE [label="Send ctx info"];
		languageServer->eclipseJdt [label="Send File Ctx Info"]; 
		eclipseJdt->languageServer [label="Send java ctx info"];
		
		languageServer[label=<Language Server>];
		eclipseJdt[label=<Eclipse Java <br/> Development Toolkit>];
	}
	
	\caption{Eclipse's Java Language Server}
	
\end{figure}

We are not interested in providing Java Language completion suggestions, as is the example case; instead, our case focuses on providing context information, with this sprint requiring the ability to display mock implementation examples.

Replacing the ability to provide Java language suggestions as in the example with that of global context information, a way forward is visible.

\begin{figure}[h!]
	
	\digraph[width=\textwidth]{ctxResolverInfo}{rankdir=LR; 
		IDE->languageServer [label="Push Ctx"];
		languageServer->IDE [label="Send ctx info"];
		languageServer->ctxProvider [label="Extracted Ctx Info"]; 
		ctxProvider->languageServer [label="Resolved examples for ctx"];
		
		ctxProvider[label=<Context Provider>];
		languageServer[label=<Language Server>];
	}
	\caption{Using a language server as middleware to resolve context}
\end{figure}



\subsection{How effective is this approach?}

We note that this approach suffers from multiple shortcomings. It is visibly computationally expensive to constantly react and respond to each IDE action to provide the necessary global context data. The latency taken to resolve this, running locally and with mocked data, suggests this approach as being non-viable. 

Whilst difficult to orchestrate a test on the current performance, we can hypothesise on its effect. If a developer performs several actions such as clicking, highlighting and hovering in the course of 10 seconds, say 8 events, with each request triggered requiring ~1s to respond, that is a total of 8 seconds in which the client is stuck in a state of non responsiveness; this incurs an overall time of 18 seconds. Guidelines from \citeauthor{nielsen1994usability} (\citeyear{nielsen1994usability}) indicate this is sufficient to exceed the user's flow of thought. 

Assessing against our goals, whilst we have managed to enrich the local scope with global context within the IDE, demonstrating sound feasibility, the latency, creates a natural impediment to usage. It also clearly violates our objective to reduce the overall time taken to find relevant information (Objective 1).

These learnings suggest the trial of an alternative approach; adopting a command triggered pull-based model where the information is requested when needed (Section \ref{sec:evaluatingPullModel}).

\section{Connecting an IDE - Evaluating the Pull Model} \label{sec:evaluatingPullModel}

With the performance and usability issues experienced through the push approach, implementing the Language Server Protocol, we resort to trialling a pull model (Figure \ref{fig:mvpPull}). A pull model implies that for the data to be displayed in the IDE, it must be explicitly requested.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=1\linewidth]{pull_trial_story.png}
		\caption{User Story}
	\end{subfigure}
	\begin{subfigure}[t]{0.6\textwidth}
		\includegraphics[width=1\linewidth]{pull-wireframe}
		\caption{Wireframe}
	\end{subfigure}
	\caption{Minimum Viable Product Illustration for Pull Approach}
	\label{fig:mvpPull}
\end{figure}


Requiring the end user to request information, rather than barraging them with it based on their interaction events (Figure \ref{fig:commonLSPEvents}), is theoretically expected to result in a reduction of the request rate and thus increase in performance. This approach also allows for a more rich, interactive display of results without needing to worry about excessive invasiveness or distraction.

\begin{figure}[h!]
	\centering
	\digraph[width=0.7\textwidth]{eventsLSP}{ 
		events->click;
		events->hover;
		events->highlight;
		events->find_references;
	}
	\caption{Commonly triggered events using Language Server Protocol}
	\label{fig:commonLSPEvents}
\end{figure}

To achieve this we carry some principles forward from using LSP. Instead we conserve the architectural style, but drop the implementation of a language protocol in favour of a more bare-bones request-reply HTTP service displaying our results using simple web technology. By using HTML and Javascript, we are able to leverage web technologies to provide a simpler, richer interaction. 

Web technologies can be leveraged for presentation throughout most of the common IDEs we have examined (Table \ref{table:webviewSupportByIDE}). This by nature means that we can reuse what we write, for our current instance of VSCode, across other IDEs.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		Environment & Supports Web Technology \\ 
		\hline 
		\hline 
		JetBrains IntelliJ & Yes, via JFrame WebView \\ 
		\hline 
		Visual Studio Code & Yes, via Electron WebView  \\ 
		\hline 
		Atom & Yes, via Electron WebView  \\ 
		\hline 
		Eclipse & Yes, via SWT Browser Support \\ 
		\hline 
	\end{tabular} 
	\caption{Web Technology Support by IDE}
	\label{table:webviewSupportByIDE}
\end{table}

We initially opt for no formatting and jQuery, to simplify the retrieval and updating of results in the web document we show. jQuery facilitates HTTP interaction through its Ajax library. 

On the server side, we cease all dependency on the Language Server Protocol. We conserve the previously developed integration between the application and the message bus, but replace the implemented event listeners for the LSP specification with a single HTTP JSON REST interface. 

We choose the convention of \textit{http://localhost:[port]/[topic]} for data access. Parameter for queries are appended as URI parameters rather than as a body (e.g. \textit{?paramA=value}), this includes paging of any data. Each request is synchronous and blocking until a response is fully transmitted. This blocking behaviour is identical as with the LSP approach.

\subsection{Does this offer an improvement?}

The immediate reduction of requests based on the human constraint of only being able to make one query at a time will inherently improve performance even with the presence of synchronous calls. Say the end user only requires to lookup information every 10 or 20 seconds, given there is only request per time frame, there is less opportunity to block the request queue.

\section{Connecting an IDE - Expanding the Query Approach}

We have mostly acted on a single, non-realistic, query payload for purpose of demonstrating the architecture and high level process. Information retrieval itself is a matter subjective to a given context with a particular associated query for what needs extracting for that context. 

The context and the query is provided to those who have mined a given motif so that they can respond with the most tailored response to address the request's need. As such it is imperative to find a way of effectively and efficiently communicating the current scenario de the developer is working on.

\subsection{So how do we model a context?}

Modelling a context object that would address every single need is difficult. Evolving an unstable API until we take into account the Pareto ratio of use cases, less so. 

Each IDE will naturally have a varying level of intelligence and many will be able to provide these attributes without any implementation effort required. One must consider the lowest common denominator in order to avoid any unnecessary normalising implementation of the same logic across each IDE; for example, the URI of an open file and folders (Table \ref{table:openFolderFileUriSupportByIDE}), allowing the language server to bare the burden of enriching these to produce a context object. 

Repository mining is often done with a Version Control System as a source. A natural choice given this is the point of integration of many other systems. Sharing the reference to individual projects in a VCS seems a viable starting  point. We chose Git, based on its dominance as the go-to VCS in current practises. 

Git repositories are formed from a root folder containing a series of files denoting the historical graphs of changes. This exists alongside the actual current version of the files which the developer is manipulating. We are able to deduce the existence of a repository by looking for this special folder containing a graph.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		Environment & Supports Open File and Folder URI \\ 
		\hline 
		\hline 
		JetBrains IntelliJ & Yes\\ 
		\hline 
		Visual Studio Code & Yes \\ 
		\hline 
		Atom & Yes  \\ 
		\hline 
		Eclipse & Yes \\ 
		\hline 
	\end{tabular} 
	\caption{Open Folder and File URI Support by IDE}
	\label{table:openFolderFileUriSupportByIDE}
\end{table}

Based on this common denominator we are able to produce a context domain model (Figure: \ref{fig:contextObject}) which can be created and enriched through the local daemon (Figure: \ref{fig:contextCreationProcess}) when queries are received.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{context-object.png}
	\caption{Query Context Model}
	\label{fig:contextObject}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{context-creation-process.png}
	\caption{Context and Query Creation Process}
	\label{fig:contextCreationProcess}
\end{figure}
	
\section{Applying to Real Data - GitHub Work Items}

We now have a good foundation to integrate real data and ultimately prove the true viability and ecological validity of this tool we are developing. To do so we must start considering an extremely broad taxonomical structure of reasons why developers interact with external tools during problem solving problem for greater insight. 

% TODO: user stories add them here....

\subsection{What are work items?}

We consider the introduction of the notion of Work Items. Work Items are considered to be an entity that documents the existence of a task or running activity which is user driven. This is broad and may include general development work tracking systems, incident tracking systems, change management systems and others.

\subsection{Why GitHub?}

Whilst integrating with academic tools would be the ideal scenario, we have been unable to find published tools with well defined APIs which we could integrate with; as such, we revert back to GitHub, which also supplies mined information, based on interactions with the Git repositories that it stores and manages, through a well documented, accessible API.

\subsection{What do we interpret as work items for Github?}

As discussed, what are constituted as Work Items varies enormously according to the bounded context in which they are situated in. For GitHub, we interpret Work Items to be synonymous with GitHub issues (Figure: \ref{fig:githubIssuesExample}). GitHub issues are the platform's way of expressing development work to do and filing bug reports.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{github-issues.png}
	\caption{GitHub Issues Example}
	\label{fig:githubIssuesExample}

\end{figure}

\subsection{What are the alternative approaches to retrieving this data?}

To retrieve this information, those requiring it will either discover it through searching for similar problems via a search engine, such as Google or more commonly deliberately navigate to the project through GitHub, then to the issues and ultimately use the filtering system to find the issue that corresponds to them; this of course does not account for the initial difficulty in finding where this is stored.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{finding-issues-flowchart.png}
	\caption{Context and Query Creation Process}
	\label{fig:findingInformationStandardWay}
\end{figure}

The alternative to this is installing a plugin that individually supports integration with GitHub tracking; however, these require manual configuration with the project to which needs integrating and often clunky in unnecessary features.

\subsection{How do we realise the integration?}

We build on the evaluated, effective pull approach. This approach can be appreciated in Figure \ref{fig:gitHubWorkItemsWireframe}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{work-items-wireframe.png}
	\caption{GitHub Work Items Results Wireframe}
	\label{fig:gitHubWorkItemsWireframe}
\end{figure}

Using the approach discussed initially in Figure \ref{fig:e2eMockClass}, we must consider the ability to support the new notion of Work Items for queries and responses. Thus we elaborate models (Figure: \ref{fig:workItemsUMLModels}) to support these. In addition, to facilitate integration, we provide a contractual interface.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{work-items-class-uml.png}
	\caption{Work Items UML Models}
	\label{fig:workItemsUMLModels}
\end{figure}

The issue with responding for any query for work items is that we may not be interested or able to respond. As such we verify that 

We build on good practise \parencite{gamma1995design} \parencite{martin2009clean} by providing integration test to ensure the integrity of the expectations and assumptions made when consuming GitHub's API. An integration test is run periodically against an API on every version change and, or on a periodic basis to ensure that the changes do not affect how one is using the API.

However, another common issue with integration is catering for constantly changing APIs on multiple fronts; if a consumer A needs something from provider B, if provider B changes the format in which they offer the language, consumer A must now adapt to this change. In a large system where there are multiple consumers or providers, this can rapidly become a problem. In Software Engineering we refer to this dilemma as high coupling, a highly undesired property. High coupling can be remedied through intermediate abstractions, high level description of what is needed from each component. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{abstraction-of-github-workitems.png}
	\caption{Work Items UML Models}
	\label{fig:abstractionOfGitHubWorkItems}
\end{figure}

To achieve these abstractions in our case, we define the expected API model provided for GitHub Issues, an interface which defines what we want from this and then ultimately use this abstraction to convert our results into the Work Item Model.

This provides us a clean integration approach as can be seen in Figure \ref{fig:gitHubworkItemsRetrievalProcess}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{github-workitems-retrieval-process.png}
	\caption{GitHub Work Items Retrieval Process}
	\label{fig:gitHubworkItemsRetrievalProcess}
\end{figure}

Last we need to enable support in the daemon to perform this query as well as the IDE integration point so that this new type of query can be presented to the user. To do so, we amend the implementation of \ref{fig:e2eMockClass} to support the newly developed models along with the corresponding jQuery client calls.

\subsection{Is this quicker than retrieving the work items manually?}

To measure the effectiveness, we set up a brief experiment. We establish an input of several work item topics that must be explored in order to aid solving a problem. We then time the time taken to locate relevant information via the traditional and our implemented approach.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{time-taken-to-retrieve-workitems-gh-boxplot}
	\caption{Graph illustrating time taken to retrieve work items from GitHub via two methods}
	\label{fig:time-taken-to-retrieve-workitems-gh-boxplot}
\end{figure}

From the plotted graph (Figure: \ref{fig:time-taken-to-retrieve-workitems-gh-boxplot}), we note two key differences. First, the time taken to retrieve work items via the traditional approach, varies vastly whilst the retrieval of work items via our implemented approach shows significant less spread and thus more consistency. Second, the new approach is visibly, approximately 2 times on average, faster when compared to the traditional approach. 

\section{Applying to Real Data - GitLab Work Items}

To prove that multiple sources can be handled simultaneously, we choose to integrate another source that provides a clear defined API for providing its own interpretation of Work Items. At times, Git repositories may have multiple remotes, which can result in Work Items being distributed across these. We choose GitLab with the same criteria as we chose GitHub: its cleanly defined API.

% TODO: user stories add them here....

\subsection{What do we interpret as work items for GitLab?}

GitLab, just alike GitHub provides issues for documenting development work and software bugs. We will provide these in response to queries for Work Items.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{gitlab-issues.png}
	\label{fig:gitlabIssuesExample}
	\caption{Example of GitLab Issues Interaction}
\end{figure}

\subsection{Enabling the integration of Work Items}

We have performed all the necessary modelling when previously exploring our approach with GitHub Issues as Work Items, and, as such only have the burden of providing the integration itself. This provides a good moment to explore the true difficulty of integrating new services with the existing system.

Using the same approach as initially defined for retrieving items, we leverage the Scatter Gather pattern which combined with the aggregator pattern enables these query results to be joined. We apply exactly the same integration testing approach and domain abstraction layer.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{abstraction-of-gitlab-workitems.png}
	\caption{Abstraction of GitLab Work Items}
	\label{fig:abstractionOfGitLabWorkItems}
\end{figure}

\subsection{What are the alternative approaches to retrieving this data?}

The approach to retrieving Issues for GitLab is equally as lengthy as that of GitHub (Figure: \ref{fig:gitHubworkItemsRetrievalProcess}). This further highlights the time consuming nature if hypothetically one had to check both sites for issues.

Another pattern that emerges is the necessity of a plugin or more per platform, which he hypothetically explored in the background section.

\subsection{Is this quicker than the alternative?}

To evaluate whether the improvements are applicable across multiple sources, we orchestrate the same experiment as we did to measure the effect our efforts had on the retrieval of work items from GitHub. We then proceed to plot these on a box plot for comparison (Figure: \ref{fig:time-to-retrieve-work-items-gl-boxplot}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{time-to-retrieve-work-items-gl-boxplot}
	\caption{Time taken to retrieve Work Items from GitLab using traditional vs our approach}
	\label{fig:time-to-retrieve-work-items-gl-boxplot}
\end{figure}

The first thing that immediately noticeable is the spread of times (range 35) is far greater in comparison to that of GitHub (range 30) when using the traditional approach. The higher median times are likely indicative on a difference in indexing and user interfaces when accessing the page directly versus its indexed result in search engines. GitHub, for example, provides easy access to projects from its landing page, whereas GitLab, this is not as explicit (Figure \ref{fig:comparisonOfLandingPagesForGhVsGL}).

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=1\linewidth]{github-landing-page.png}
		\caption{GitHub}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=1\linewidth]{gitlab-landing-page.png}
		\caption{GitLab}
	\end{subfigure}
	\caption{Comparison of Landing Pages}
	\label{fig:comparisonOfLandingPagesForGhVsGL}
\end{figure}

Not withstanding the differences in time for the traditional approach we note that appearance that the effectiveness of our solution is transferable across sources, with a median for both of 18 seconds to retrieve the needed information.

\subsection{How simple is integrating other sources?}

In terms of the awful measurement of Lines of code, very simple. Difficulty wise, this is not very complex either. We could facilitate an even simpler approach through the use of code scaffolding tools such as Yeoman or using an initialiser like the ``Spring Initializr'' so that the integrating developer has as little boilerplate code to write as possible.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{spring-initializr.png}
	\label{fig:springExample}
	\caption{An example of a generation tool by Spring}
\end{figure}

\subsection{What are the key issues uncovered here?}

As the number of components has grown, so has the complexity of orchestrating installation and configuration of the server side components. The previous utilisation of Docker Compose, reveals clear weaknesses. The gold-standard of container orchestration is seen as Kubernetes \parencite{kuberenetesOfficialSite}. Kubernetes is highly configurable, but allows for exact, immutable, replications of infrastructure per a declaration of the Infrastructure as Code.

Packaging any application is a key challenge of facilitating development and, or distribution. Openshift \parencite{openshiftSite} and Helm \parencite{helmOnlineResource} each provide their own approach to packaging Kubernetes deployments. We choose Helm due to its closeness to vanilla Kubernetes and its ease of use. 

\section{Applying to Real Data - GitHub Experts}

We've proven the effectiveness and viability of the solution when applied to documenting Work Items. We must provide an expansion to the available motifs or topics to cater for a wider variety of typical developer queries.


% TODO: user stories add them here....

\subsection{What are experts?}

Cambridge Dictionary defines an expert as ``a person with a high level of knowledge or a skill relating to a particular subject or activity'' \citedate{cambrdigeDictionaryExpert}.

We build on this suggesting that in our context, people are considered to have expertise in a given domain, if they participate in documenting, coding, feature or defect discussions or similar; but we strongly emphasize the subjective nature of this and its openness to interpretation by the provider.

\subsection{What do we interpret as experts in GitHub?}

We choose to rely on GitHub yet again because it has a well defined API. GitHub itself uses Git and project dependency means to provide an insight into those who have participated in a project \footnote{\url{https://help.github.com/en/github/visualizing-repository-data-with-graphs/viewing-a-projects-contributors}}. The platform refers to these as contributors and provides API access to these.

\subsection{How do we realise this integration?}

This is an entirely new topic or motif that does not exist as part of our modelling, requiring us to evolve one. For now we model an Expert as someone with a name, a particular mode of contact and a set of topics or reasons as to why they are considered to be an expert (Figure: \ref{fig:expertsUmlClassModel}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{experts-uml-class-model.png}
	\caption{UML Class Diagram for Enabling Experts Queries}
	\label{fig:expertsUmlClassModel}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{experts-wireframe.png}
	\caption{Wireframe modelling of Experts presentation in IDE}
	\label{fig:expertsWireframe}
\end{figure}

Without value, we skip explaining the approach to testing, good integration practises as well as providing an integration layer.

\subsection{What are the alternative approaches to retrieving this data?}

The approaches to retrieving this are similar to those discussed in previous sections with the exception that some of the information mined here can be obtained by directly invoking Git's blame feature \footnote{\url{https://git-scm.com/docs/git-blame}} or Git's Shortlog command.

\subsection{What are the improvements over the alternative?}

To evaluate the improvements, similar to the case of work items, we set up an experiment to measure the time taken to retrieve experts for a particular project. We measure this time for retrieving experts via the traditional search engine or direct access, git blame or shortlog command, and our solution. We then plot these results using a box plot to illustrate the distribution (Figure: \ref{fig:time-to-retrieve-experts-gh-boxplot}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{time-to-retrieve-experts-gh-boxplot}
	\caption{Comparison of time taken to retrieve experts for a particular project via our solution, traditional approach and git commands}
	\label{fig:time-to-retrieve-experts-gh-boxplot}
\end{figure}

Observing the above results we can see, yet again, that our solution performs significantly better in comparison to the traditional approach. However, is beaten by commands provided by the version control tool. We note that the approach to obtain the same information from the version control tool has a far steeper learning curve, so this must be considered.

\section{Addressing real world performance issues}

There is noticeable performance hit when exploring the retrieval of work items from repositories, in particular GitHub, when there is a large number of repositories to retrieve even despite the usage of pagination. This performance issue risks compromising the goal of reducing the overall time taken to find relevant information for a given problem context. This issue was not uncovered in testing as the test repositories were not of great size.

\subsection{How do we identify and remove the bottlenecks?}

To identify the bottlenecks in our performance, we start by focusing on the slowest part of the processing chain: the retrieval from GitHub itself. We make the assumption that this will be the slowest based on the network distance travelled.

We first examine the integration point between the mining application and the data source itself; in our case, this is an existing third party GitHub API library. Through observation, we note that the control over caching and pagination lacks granularity, reducing our ability to fine tune. We replace this with pure Spring based REST HTTP calls, to enable finer control over our interaction with the data source.

The next step we focus on eliminating state from pipeline we have established. To do so, we revert from a traditional object-oriented approach towards a streamlined use of the Functional paradigm enabled by the Java Streams API.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{ide-daemon-protocol-improved.png}
	\caption{UML Sequence Diagram Illustrating client-side improvements}
	\label{fig:ideDaemonProtocolImproved}
\end{figure}

We then move to reducing the bottlenecks on the client-running applications. Within the Context Daemon, rather than supplying the results directly from the initial request from the IDE, we opt for a more structured approach with a slightly more complex protocol. This is done by substituting a response of the results in the initial request with a results aggregate identifier which can then be repeatedly queried in successive calls (Figure \ref{fig:ideDaemonProtocolImproved}).

This beneficially results in the adoption of a multiple-faceted buffer for the transfer of data across the system, allowing the system to cater for the varying speeds between its different components.

\subsection{Did these changes offer improvement?}

To orchestrate an experiment, we take the two artefacts which have been modified to improve the performance, at a state before the changes were made and after the improvements were made. We then orchestrate a repeated test, through a Python script that repeats a retrieval 50 times, recording the time taken each time. We then plot this on a boxplot to show the distribution of the results (Figure \ref{fig:time-taken-to-display-first-result})

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{time-taken-to-display-first-result}
	\caption{Comparison of time taken to display first result before and after improvements made}
	\label{fig:time-taken-to-display-first-result}
\end{figure}

Examining the results, we note the extremely rapid and consistent retrieval time obtained by the improvements, in which a sub-second retrieval time is appreciable. This is consistent with the UX requirements explored before.

\section{Applying to Real Data - Stack Overflow Documentation}

It is imperative to have one last form of common developer resource to illustrate and validate diverse application of the approach we are taking as well as explore any potential issues that may arise as a result of integrating multiple sources. Documentation in itself is a common action in almost any engineering discipline as complex domains require referential material to understand.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{stack-overflow-documentation.png}
	\caption{Example of a Stack Overflow response}
	\label{fig:stackOverflowDocSite}
\end{figure}

\subsection{What is documentation?}

Documentation is considered to be anything that illustrates how a system, component, process or other works whether from an abstract point of view or concrete as may be code examples, test cases - specification by example - or questions and answers.

\subsection{Why Stack overflow?}

It is entirely unfeasible to expect a developer to remember, or know how to use every system they interact with, or for that matter anyone to use anything of any complicated nature without reference to fall back to. Developer's often resort to Question-based sites to rapidly solve cases by example. Stack Overflow is a popular choice for this approach to problem solving; it exposes a limited use, well documented public API that we will use to demonstrate the effectiveness.

\subsection{What are the alternative approaches to retrieving this data?}

Generally, this approach to problem has multiple heuristic approaches; these include: copy-pasting lines of code, copying error messages, or even general keywords that are likely to return a result. 

\subsection{The Implementation}

Following the same good practise principles, such as integration testing, that we have for each integration of new motifs, we begin implementing support for the new motif of documentation and the exemplar source. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{documentation-uml-model.png}
	\caption{UML Class Diagram for Enabling Documentation Queries}
	\label{fig:documentationUmlClass}
\end{figure}

Next, we extend the existing user-interface implementation to allow for documentation (Figure: \ref{fig:documentationWireframe}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{documentation-wireframe.png}
	\caption{Documentation Wireframe}
	\label{fig:documentationWireframe}
\end{figure}

\subsection{What are the improvements over this?}

The results retrieving from forwarding certain code snippets directly to the StackOverflow API do not always return useful, entirely relevant results. We consider that these refinements are beyond the scope of the objectives set, given we merely aim to prove the possibility of these integration and not explore the intricacies of mining itself.

However, the lack of relevant results from a single API, does not mean that the integration itself does not serve a purpose. Instead, we focus on evaluating the time taken to reach the same results through the traditional method and current method.

To carry out a measurement of this, we orchestrate a simple experiment. The experiment consists in searching for examples that are relevant to a context by searching for code and comments that we highlight in our IDE. We repeat the experiment using our implemented approach and then using the traditional means through a search engine and time the time taken to obtain results.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{time-to-retrieve-documentation}
	\caption{Comparison of time taken to retrieve documentation for implemented solution vs traditional approach}
	\label{fig:time-to-retrieve-documentation}
\end{figure}

%TODO;

\section{Refining Presentation for Extensibility}

We have largely focused on the modularity of the server-side system, and have emphasized little interest or importance on the developed user-interface, which as topics are added, becomes increasingly more difficult to maintain and evolve. This is opposite to our goal of a modular, easily extensible solution.

\subsection{What is the new approach?}

We have described Microservices as the solution to modularity, and all its benefits, in a distributed system. However, this does not account for the fact that ultimately users interact with a single user interface that is developed monolithic in itself. In recent years, Web Components, have grown to address the growing need to address the bottleneck faced in UI development. Component-driven development is now the norm in major web-frameworks (Figure: \ref{table:webFrameworkComponentSupport}).

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		Framework & Supports Components \\ 
		\hline 
		\hline 
		Angular & Yes \\ 
		\hline
		ReactJS & Yes \\ 
		\hline 
		VueJS & Yes \\ 
		\hline
		Polymer & Yes \\ 
		\hline  
	\end{tabular} 
	\caption{Web Frameworks and Support for Component-based Development}
	\label{table:webFrameworkComponentSupport}
\end{table}

As a replacement for the raw jQuery and Ajax approach initially used to implement the technology, we opt for Angular. This decision is based on its well defined, CLI-scaffolded \footnote{Command Line Interface prompted assistance for creating / modifying applications} approach that offers a clean, intuitive development for web applications.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{modular-ui.png}
	\caption{Component Diagram Illustrating Modular UI}
	\label{fig:modularUIComponentDiagram}
\end{figure}


We must consider the need to cleanly interface with the events and requests from  any IDE as well as provide a clean separation of concerns for each type of data we display so that each is entirely independent from another. We can achieve this by providing an abstraction layer within the Web View that is capable of translating events from the varying different extension hosts to the Web View - generally through IPC \footnote{Inter Process Communication - View and Host are independent processes}. For each integration type, a component is developed an allocated (Figure: \ref{fig:modularUIComponentDiagram}). 

\subsection{Does this new approach offer improvements?}

With this approach, the overall coupling between components is reduced as well as the cohesiveness increased. This is difficult to measure with existing tools given the enormous difference between both implementations used, however can easily be explored with sound reasoning. 

%TODO:

\chapter{Evaluation}

To summarise, we proceed to draw together the conclusions. 