\chapter{Iterations}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{iterations-diagram.png}
	\caption{Brief overview of the contents covered in each iteration}
\end{figure}

\newpage
\section{Establishing the Architectural Plumbing}

There are two choices when it comes to creating modular applications, either a plugin based system, such as Javaâ€™s OSGi standard, or independent applications that communicate over some form of local or remote connection.

Naturally, this poses questions as to what the nature of scale of the problem would be; in our case, tying multiple information sources together, typical of a medium size corporation do we lean to the solution that better applies to a larger scale.

Extensibility in our case is important; there is an inexorable need for addition of new sources. Plugins are the inherent approach to providing modular extension to a monolithic architecture (Figure \ref{fig:monolithic-approach}). Whereas, as a superset of Service Oriented Architecture, Micro-services are more strongly decoupled (Figure \ref{fig:distributed-approach}), relying on middleware to communicate based on some form of predefined contract.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{monolithic-approach.png}
	\caption{Component Diagram Illustrating Monolithic Approach}
	\label{fig:monolithic-approach}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{distributed-application.png}
	\caption{Component Diagram Illustrating Distributed Approach}
	\label{fig:distributed-approach}
\end{figure}

With large amounts of data exchanged - as is the potential in the envisioned system - a single monolithic application acting as the single processor of the data can rapidly become the chokepoint, subject to temporary mediation via vertical scaling. In contrast, these problems are easily mitigated in distributed architecture through horizontal scaling.

Microservices are no silver bullet approach to proverbial modularity and, or business scalability. In fact, they are often associated with greater development overhead, especially, in the aspect of integration and testing due to lack of immediate feedback on changes; the appropriation of the correct culture, practises and tooling can mitigate many of these overheads. Failing these appropriations, other common anti patterns may emerge, such as distributed monoliths. We will discuss these later.

There are many integration patterns for distributed systems. EIP - a referential principle-driven manual - notes their shortcomings and benefits, but strongly recommends messaging integration patterns as the best approach for most Enterprise problems favouring them based on their high frequency, asynchronous capability.

Implementing a fully functional, distributed messaging solution is complex endeavour; reinventing the wheel is futile when there are ample choices to choose from; RabbitMQ, Apache Kafka, to name a few. Not withstanding the reduction of unnecessary development effort, from the elimination of implementing middleware, duct tape code is still necessary to handle the endpoints connecting to these.

The JVM and its associated languages, are ubiquitous in back office systems in many businesses across the globe; this maturity yields stage to many out-of-the-box solutions to common enterprise problems such as that which we have discussed. Spring is an example of one of the most widely adopted enterprise Java frameworks facilitating these aids.

Spring itself does not however dictate the style of distributed programming but offers a strongly opinionated view on the approaches available. We chose the Command Query Responsibility Segregation (CQRS) approach based on its focus on separation of updating a set of projections - query models - and updating of a central decoupled model - command models. Implementing this pattern is infamously difficult, but with help of an extension based on Spring - Axon Framework - can we reduce much of this effort.  

With the choice of Axon come some beneficial consequences. The framework provides a strongly opinionated view of what is best for the developer; its strong point lays in its approach to Messaging. It defines its own Message Bus implementation - which can be substituted from a set of other choices if needed - as well as handling all of the boiler-plating that would otherwise be needed to glue message endpoint consumption together.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{distributed-approach-with-axon.png}
	\caption{Component Diagram Illustrating Distributed Approach with Axon Framework}
	\label{fig:distributed-approach}
\end{figure}

\section{Establishing a Retrieval Mechanism}

Having established, through thorough evaluation, a sound architecture, we proceed to establishing our approach to how data is expected to flow between the defined components. Having defined, and implemented a rough architecture above, we resort to implementing a minimum viable product for retrieving from a client for which we choose a HTTP Web Approach with a single test parameter in the pursuit of a creation of query handler that will return a basic hello world message returning the test parameter appended. Following this approach, we are able to create a very simple test case to verify the functionality (Figure \ref{fig:e2eMockTest}).

\begin{figure}[h!]
		\centering
		\lstinputlisting[language=Python, showspaces=false,                
		showstringspaces=false, tabsize=2]{e2e-acceptance-test-1.py}
		\caption{End To End Test for Mock Example}
		\label{fig:e2eMockTest}
\end{figure}

To achieve the functionality, we define a query object capable of transmitting the test payload. The query object also serves as the contract for those wishing the act on it based on its object signature. We model this process in Figure. Clients receive a query through an interface which they dispatch with context to the message bus and which is then returned from the appropriate source. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{first_e2e_test_class_uml.png}
	\caption{UML Class Diagram of the models involved in building and responding to a query}
	\label{fig:firste2etestclassuml}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{process-diagram.png}
	\caption{Process diagram illustrating flow of query from its handling to response}
	\label{fig:processSequenceQueryFlow}
\end{figure}

By implementing this approach, we end up with a core code example of what will form the implementation pattern throughout this project (Figure \ref{fig:e2eMockClass}). The simplicity of this approach is evident and strongly contributes to our goal of ease of extensibility. The complexity of deploying this distributed application is handled through the use of containerisation - Docker - and container orchestration - Docker Compose. 

Despite not performing much processing, the response times indicate a positive approach to reducing the time taken to retrieve information.

% TODO: insert graph here.

% TODO: what are the response times like here?

\begin{figure}[h!]
	\centering
	\lstinputlisting[language=Java, showspaces=false,                
	showstringspaces=false, tabsize=2, breaklines=true]{mock-e2e.java}
	\caption{End To End Implementation for Mock Example}
	\label{fig:e2eMockClass}
\end{figure}

\section{Connecting an IDE - Evaluating the Push Model}

In the previous section, we successfully managed to establish a basic decoupled communication model that enabled the easy integration of information from a given source. In line with the background research, we have a key interest in reducing the amount of context switching between multiple sources performed by a developer and as such found the IDE to be a suitable approach. We also explored the relevance of simple problems such as the Language Server Protocol. 

We now explore the extension of our current solution in pursuit of the ability to connect it to an IDE. In this case, Visual Studio Code. 

When thinking of creating reusable IDE Intelligence that can used to enrich context based on a given syntax and or grammar, we may consider the Language Server protocol. 

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=1\linewidth]{push_trial_story.png}
		\caption{User Story}
	\end{subfigure}
	\begin{subfigure}[t]{0.6\textwidth}
		\includegraphics[width=1\linewidth]{push-wireframe}
		\caption{Wireframe}
	\end{subfigure}
	\label{fig:mvpPush}
	\caption{Minimum Viable Product Illustration for Push Approach}
\end{figure}

We may examine examples of its usage, such as Eclipse's Java LSP implementation, which uses the Eclipse Java Development Tools to provide Java support to all LSP supporting IDEs.

\begin{figure}[h!]
	
	\digraph[width=\textwidth]{eclipseLanguageServer}{rankdir=LR; 
		IDE->languageServer [label="Push Ctx"];
		languageServer->IDE [label="Send ctx info"];
		languageServer->eclipseJdt [label="Send File Ctx Info"]; 
		eclipseJdt->languageServer [label="Send java ctx info"];
		
		languageServer[label=<Language Server>];
		eclipseJdt[label=<Eclipse Java <br/> Development Toolkit>];
	}
	
	\caption{Eclipse's Java Language Server}
	
\end{figure}

We are not interested in providing Java Language completion suggestions, as is the example case; instead, our case focuses on providing context information, with this sprint requiring the ability to display mock implementation examples.

Replacing the ability to provide Java language suggestions as in the example with that of global context information, a way forward is visible.

\begin{figure}[h!]
	
	\digraph[width=\textwidth]{ctxResolverInfo}{rankdir=LR; 
		IDE->languageServer [label="Push Ctx"];
		languageServer->IDE [label="Send ctx info"];
		languageServer->ctxProvider [label="Extracted Ctx Info"]; 
		ctxProvider->languageServer [label="Resolved examples for ctx"];
		
		ctxProvider[label=<Context Provider>];
		languageServer[label=<Language Server>];
	}
	\caption{Using a language server as middleware to resolve context}
\end{figure}



\subsection{So how effective is this approach?}

After demonstrating the resulting product of this sprint, we note that this approach suffers from multiple shortcomings.

It is visibly computationally expensive to constantly react and respond to each IDE action to provide the necessary global context data. The latency taken to resolve this, running locally and with mocked data, suggests this approach as being non-viable. 

Whilst difficult to orchestrate a test on the current performance, we can hypothesise on its effect. If a developer performs several actions such as clicking, highlighting and hovering in the course of 10 seconds, say 8 events, with each request triggered requiring ~1s to respond, that is a total of 8 seconds in which the client is stuck in a state of non responsiveness; this incurs an overall time of 18 seconds. Guidelines from \citeauthor{nielsen1994usability} (\citeyear{nielsen1994usability}) indicate this is sufficient to exceed the user's flow of thought. 

Assessing against at our aim, whilst we have managed to enrich a our local scope with global context within the IDE, demonstrating sound feasibility, the latency, creates a natural impediment to usage. It also clearly violates our objective to reduce the overall time taken to find relevant information (Objective 1).

These learnings suggest the trial of an alternative approach; adopting a command triggered pull-based model where the information is requested when needed.

\section{Connecting an IDE - Evaluating the Pull Model}

With the performance and usability issues experienced through the push approach, implementing the Language Server Protocol, we resort to trialling a pull model (Figure \ref{fig:mvpPull}). A pull model implies that for the data to be displayed in the IDE, it must be explicitly requested.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[width=1\linewidth]{pull_trial_story.png}
		\caption{User Story}
	\end{subfigure}
	\begin{subfigure}[t]{0.6\textwidth}
		\includegraphics[width=1\linewidth]{pull-wireframe}
		\caption{Wireframe}
	\end{subfigure}
	\caption{Minimum Viable Product Illustration for Pull Approach}
	\label{fig:mvpPull}
\end{figure}


Requiring the end user to request information, rather than barraging them with it based on their interaction events (Figure \ref{fig:commonLSPEvents}), is theoretically expected to result in a reduction of the request rate and thus increase in performance. This approach also allows for a more rich, interactive display of results without needing to worry about excessive invasiveness or distraction.

\begin{figure}[h!]
	\centering
	\digraph[width=0.7\textwidth]{eventsLSP}{ 
		events->click;
		events->hover;
		events->highlight;
		events->find_references;
	}
	\caption{Commonly triggered events using Language Server Protocol}
	\label{fig:commonLSPEvents}
\end{figure}

To achieve this we need not scrap entirely what was done previously. Instead we conserve the architectural style, but drop the implementation of a language protocol in favour of a more bare-bones request-reply HTTP service displaying our results using simple web technology. By using HTML and Javascript, we are able to leverage web technologies to provide a simpler, richer interaction. 

Web technologies can be leveraged for presentation throughout most of the common IDEs we have examined (Table \ref{table:webviewSupportByIDE}). This by nature means that we can reuse what we write, for our current instance of VSCode, across other IDEs.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		Environment & Supports Web Technology \\ 
		\hline 
		\hline 
		JetBrains IntelliJ & Yes, via JFrame WebView \\ 
		\hline 
		Visual Studio Code & Yes, via Electron WebView  \\ 
		\hline 
		Atom & Yes, via Electron WebView  \\ 
		\hline 
		Eclipse & Yes, via SWT Browser Support \\ 
		\hline 
	\end{tabular} 
	\caption{Web Technology Support by IDE}
	\label{table:webviewSupportByIDE}
\end{table}

We initially opt for no formatting and jQuery, to simplify the retrieval and updating of results in the web document we show. jQuery facilitates HTTP interaction through its Ajax library. 

On the server side, we cease all dependency on the Language Server Protocol. We conserve the previously developed integration between the application and the message bus, but replace the implemented event listeners for the LSP specification with a single HTTP JSON REST interface. 

We choose the convention of \textit{http://localhost:[port]/[topic]} for data access. Parameter for queries are appended as URI parameters rather than as a body (e.g. \textit{?paramA=value}), this includes paging of any data. Each request is synchronous and blocking until a response is fully transmitted. This blocking behaviour is identical as with the LSP approach.

\subsection{Does this offer an improvement?}

The immediate reduction of requests based on the human constraint of only being able to make one query at a time will inherently improve performance even with the presence of synchronous calls. Say the end user only requires to lookup information every 10 or 20 seconds, given there is only request per time frame, there is less opportunity to block the request queue.

\section{Connecting an IDE - Expanding the Query Approach}

We have mostly acted on a single, non-realistic, query payload for purpose of demonstrating the architecture and high level process. Information retrieval itself is a matter subjective to a given context with a particular associated query for what needs extracting for that context. 

The context and the query is provided to those who have mined a given motif so that they can respond with the most tailored response to address the request's need. As such it is imperative to find a way of effectively and efficiently communicating the current scenario de the developer is working on.

\subsection{So how do we model a context?}

Modelling a context object that would address every single need is difficult. Evolving an unstable API until we take into account the Pareto ratio of use cases, less so. 

Each IDE will naturally have a varying level of intelligence and many will be able to provide these attributes without any implementation effort required. One must consider the lowest common denominator in order to avoid any unnecessary normalising implementation of the same logic across each IDE; for example, the URI of an open file and folders (Table \ref{table:openFolderFileUriSupportByIDE}), allowing the language server to bare the burden of enriching these to produce a context object. 

Repository mining is often done with a Version Control System as a source. A natural choice given this is the point of integration of many other systems. Sharing the reference to individual projects in a VCS seems a viable starting  point. We chose Git, based on its dominance as the go-to VCS in current practises. 

Git repositories are formed from a root folder containing a series of files denoting the historical graphs of changes. This exists alongside the actual current version of the files which the developer is manipulating. We are able to deduce the existence of a repository by looking for this special folder containing a graph.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		Environment & Supports Open File and Folder URI \\ 
		\hline 
		\hline 
		JetBrains IntelliJ & Yes\\ 
		\hline 
		Visual Studio Code & Yes \\ 
		\hline 
		Atom & Yes  \\ 
		\hline 
		Eclipse & Yes \\ 
		\hline 
	\end{tabular} 
	\caption{Open Folder and File URI Support by IDE}
	\label{table:openFolderFileUriSupportByIDE}
\end{table}

Based on this common denominator we are able to produce a context domain model (Figure: \ref{fig:contextObject}) which can be created and enriched through the local daemon (Figure: \ref{fig:contextCreationProcess}) when queries are received.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{context-object.png}
	\caption{Query Context Model}
	\label{fig:contextObject}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{context-creation-process.png}
	\caption{Context and Query Creation Process}
	\label{fig:contextCreationProcess}
\end{figure}
	
\section{Applying to Real Data - GitHub Work Items}

We now have a good foundation to integrate real data and ultimately prove the true viability and ecological validity of this tool we are developing. To do so we must start considering an extremely broad taxonomical structure of reasons why developers interact with external tools during problem solving problem for greater insight. 

% TODO: user stories add them here....

\subsection{What are work items?}

We consider the introduction of the notion of Work Items. Work Items are considered to be an entity that documents the existence of a task or running activity which is user driven. This is broad and may include general development work tracking systems, incident tracking systems, change management systems and others.

\subsection{Why GitHub?}

Whilst integrating with academic tools would be the ideal scenario, we have been unable to find published tools with well defined APIs which we could integrate with; as such, we revert back to GitHub, which also supplies mined information, based on interactions with the Git repositories that it stores and manages, through a well documented, accessible API.

\subsection{What do we interpret as work items for Github?}

As discussed, what are constituted as Work Items varies enormously according to the bounded context in which they are situated in. For GitHub, we interpret Work Items to be synonymous with GitHub issues (Figure: \ref{fig:githubIssuesExample}). GitHub issues are the platform's way of expressing development work to do and filing bug reports.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{github-issues.png}
	\label{fig:githubIssuesExample}
	\caption{GitHub Issues Example}

\end{figure}

\subsection{What are the alternative approaches to retrieving this data?}

To retrieve this information, those requiring it will either discover it through searching for similar problems via a search engine, such as Google or more commonly deliberately navigate to the project through GitHub, then to the issues and ultimately use the filtering system to find the issue that corresponds to them; this of course does not account for the initial difficulty in finding where this is stored.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{finding-issues-flowchart.png}
	\caption{Context and Query Creation Process}
	\label{fig:findingInformationStandardWay}
\end{figure}

The alternative to this is installing a plugin that individually supports integration with GitHub tracking; however, these require manual configuration with the project to which needs integrating and often clunky in unnecessary features.

\subsection{How do we realise the integration?}

We build on the evaluated, effective pull approach. This approach can be appreciated in Figure \ref{fig:gitHubWorkItemsWireframe}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{work-items-wireframe.png}
	\label{fig:gitHubWorkItemsWireframe}
	\caption{GitHub Work Items Results Wireframe}
\end{figure}

Using the approach discussed initially in Figure \ref{fig:e2eMockClass}, we must consider the ability to support the new notion of Work Items for queries and responses. Thus we elaborate models (Figure: \ref{fig:workItemsUMLModels}) to support these. In addition, to facilitate integration, we provide a contractual interface.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{work-items-class-uml.png}
	\caption{Work Items UML Models}
	\label{fig:workItemsUMLModels}
\end{figure}

The issue with responding for any query for work items is that we may not be interested or able to respond. As such we verify that 

We build on good practise \parencite{gamma1995design} \parencite{martin2009clean} by providing integration test to ensure the integrity of the expectations and assumptions made when consuming GitHub's API. An integration test is run periodically against an API on every version change and, or on a periodic basis to ensure that the changes do not affect how one is using the API.

However, another common issue with integration is catering for constantly changing APIs on multiple fronts; if a consumer A needs something from provider B, if provider B changes the format in which they offer the language, consumer A must now adapt to this change. In a large system where there are multiple consumers or providers, this can rapidly become a problem. In Software Engineering we refer to this dilemma as high coupling, a highly undesired property. High coupling can be remedied through intermediate abstractions, high level description of what is needed from each component. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{abstraction-of-github-workitems.png}
	\caption{Work Items UML Models}
	\label{fig:abstractionOfGitHubWorkItems}
\end{figure}

To achieve these abstractions in our case, we define the expected API model provided for GitHub Issues, an interface which defines what we want from this and then ultimately use this abstraction to convert our results into the Work Item Model.

This provides us a clean integration approach as can be seen in Figure \ref{fig:gitHubworkItemsRetrievalProcess}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{github-workitems-retrieval-process.png}
	\caption{GitHub Work Items Retrieval Process}
	\label{fig:gitHubworkItemsRetrievalProcess}
\end{figure}

Last we need to enable support in the daemon to perform this query as well as the IDE integration point so that this new type of query can be presented to the user. To do so, we amend the implementation of \ref{fig:e2eMockClass} to support the newly developed models along with the corresponding jQuery client calls.

\subsection{Is this quicker than retrieving the work items manually?}

Yes, but why?

Repeat manual approaches we conducted show the traditional approach taking a mean time of circa 42 seconds whereas with the implementation this is cut down to a mean time circa 12 seconds. 

Could we use a box plot to show this?

relate back to how this affects objective... 

this is currently strong affected by the inability to perform observational studies because of COVID-19, consult tutor on how to proceed; we could just show own manual repetition as evaluation.

\section{Applying to Real Data - GitLab Work Items}

To prove that multiple sources can be handled simultaneously, we choose to integrate another source that provides a clear defined API for providing its own interpretation of Work Items. At times, Git repositories may have multiple remotes, which can result in Work Items being distributed across these. We choose GitLab with the same criteria as we chose GitHub: its cleanly defined API.

% TODO: user stories add them here....

\subsection{What do we interpret as work items for GitLab?}

GitLab, just alike GitHub provides issues for documenting development work and software bugs. We will provide these in response to queries for Work Items.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{gitlab-issues.png}
	\label{fig:gitlabIssuesExample}
	\caption{Example of GitLab Issues Interaction}
\end{figure}

\subsection{How do we do this?}

We have performed all the necessary modelling when previously exploring our approach with GitHub Issues as Work Items, and, as such only have the burden of providing the integration itself. This provides a good moment to explore the true difficulty of integrating new services with the existing system.

Using the same approach as initially defined for retrieving items, we leverage the Scatter Gather pattern which combined with the aggregator pattern enables these query results to be joined. We apply exactly the same integration testing approach and domain abstraction layer.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{abstraction-of-gitlab-workitems.png}
	\caption{Abstraction of GitLab Work Items}
	\label{fig:abstractionOfGitLabWorkItems}
\end{figure}

\subsection{What are the alternative approaches to retrieving this data?}

The approach to retrieving Issues for GitLab is equally as lengthy as that of GitHub (Figure: \ref{fig:gitHubworkItemsRetrievalProcess}). This further highlights the time consuming nature if hypothetically one had to check both sites for issues.

Another pattern that emerges is the necessity of a plugin per platform, which he hypothetically explored in the background section.

\subsection{Is this quicker than the alternative?}

Yes, but why?

\subsection{How simple is integrating other sources?}

In terms of the awful measurement of Lines of code, very simple. Difficulty wise, this is not very complex either. We could facilitate an even simpler approach through the use of code scaffolding tools such as Yeoman or using an initialiser like the ``Spring Initializr'' so that the integrating developer has as little boilerplate code to write as possible.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{spring-initializr.png}
	\label{fig:springExample}
	\caption{An example of a generation tool by Spring}
\end{figure}

\subsection{What are the key issues uncovered here?}

As the number of components has grown, so has the complexity of orchestrating installation and configuration of the server side components. 

\section{Applying to Real Data - GitHub Experts}

We've proven the effectiveness and viability and effectiveness of the solution when applied to documenting Work Items. We must provide an expansion to the available motifs or topics to cater for a wider variety of typical developer queries.


% TODO: user stories add them here....

\subsection{What are experts?}

Cambridge Dictionary defines an expert as ``a person with a high level of knowledge or a skill relating to a particular subject or activity'' \citedate{cambrdigeDictionaryExpert}.

We build on this suggesting that in our context, people are considered to have expertise in a given domain, if they participate in documenting, coding, feature or defect discussions or similar; but we strongly emphasize the subjective nature of this and its openness to interpretation by the provider.

\subsection{What do we interpret as experts in GitHub?}

We choose to rely on GitHub yet again because it has a well defined API. GitHub itself uses Git and project dependency means to provide an insight into those who have participated in a project \footnote{\url{https://help.github.com/en/github/visualizing-repository-data-with-graphs/viewing-a-projects-contributors}}. The platform refers to these as contributors and provides API access to these.

\subsection{How do we realise this integration?}

This is an entirely new topic or motif that does not exist as part of our modelling, requiring us to evolve one. For now we model an Expert as someone with a name, a particular mode of contact and a set of topics or reasons as to why they are considered to be an expert (Figure: \ref{fig:expertsUmlClassModel}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{experts-uml-class-model.png}
	\label{fig:expertsUmlClassModel}
	\caption{UML Class Diagram for Enabling Experts Queries}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{experts-wireframe.png}
	\caption{Wireframe modelling of Experts presentation in IDE}
	\label{fig:expertsWireframe}
\end{figure}

Without value, we skip explaining the approach to testing, good integration practises as well as providing an integration layer.

\subsection{What are the alternative approaches to retrieving this data?}

The approaches to retrieving this are similar to those discussed in previous sections with the exception that some of the information mined here can be obtained by directly invoking Git's blame feature \footnote{\url{https://git-scm.com/docs/git-blame}}.

\subsection{What are the improvements over the alternative?}

Quicker to access.

\section{Addressing real world performance issues}

- Why is this the next step?

Because the performance sucks seriously when retrieving repositories with large amounts of results to the extent that pagination doesn't appear to fix. It's an impediment to achieving multiple goals.

\subsection{What's the current performance?}

Absolutely awful. Insert graph showing .4.0 0 performance of ~+30s for a large query.

\subsection{How do we identify the choke points?}

Heuristically working from the provider of the data backwards.

Replaced use of API libraries with raw calls to remove caching for GitHub.

Multiple buffer points. E.g. between IDE - Daemon - Resource - API.

\subsection{Did we improve?}

Show graph showing 0.6.1 performance vs 0.4.0.

\section{Applying to Real Data - Stack Overflow Documentation}

- Why is this the next step?

Need to have one last form of common developer resource to illustrate and validate diverse application of the approach and explore any potential issues that may arise as a result of integrating multiple sources.

\subsection{What is documentation?}

Documentation is considered to be anything that illustrates how a system, component, process or other works whether from an abstract point of view or concrete as may be code examples, test cases or questions and answers.

\subsection{Why Stack overflow?}

Well referred to resource by developers for solving. Provides an open API that is easy to interact with.

\subsection{What are the alternative approaches to retrieving this data?}

\subsection{What are the improvements over this?}

\section{Refining Presentation for Extensibility}

- Why this step next?

We have largely focused on the modularity of the server-side system, with little interest on the actual developed UI which as topics are added becomes increasingly more difficult to maintain and evolve.

\subsection{What is the new approach?}

Component driven web framework in replacement of jQuery. 

Which framework though? React vs Angular vs Vue vs Polymer.

\subsection{Does this new approach offer improvements?}

Yes! Contributes to improved modularity. Separation of concerns is far better improved, greater structure, assistance of angular scaffolding tools etc makes an enormous difference.

%\input{iterations/sprint1.tex}
%\input{iterations/sprint2.tex}
%\input{iterations/sprint3.tex}
%\input{iterations/sprint4.tex}
%\input{iterations/sprint5.tex}
%\input{iterations/sprint6.tex}
%\input{iterations/sprint7.tex}
%\input{iterations/sprint8.tex}